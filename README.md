# Large Dataset Processing with Pandas, Dask, and Modin

## Overview

This project benchmarks the processing of large datasets (over 2GB) using pandas, Dask, and Modin, focusing on read performance, data validation, and preprocessing efficiency. It dynamically handles dataset configurations through a YAML file, ensuring flexibility and scalability in data processing workflows.

## Dataset

Download the "PUBG Match Deaths and Statistics" dataset from Kaggle and place it in a known directory within the project.
https://www.kaggle.com/datasets/skihikingkevin/pubg-match-deaths

![djzln9jnhrez](https://github.com/N0VA-code/Large-Dataset-Processing-with-Pandas-Dask-and-Modin/assets/80535193/d592485a-16bc-4bf8-a1c6-2bfa3a49e930)

### Prerequisites

- Python 3.x
- Pandas
- Dask
- Modin
- PyYAML

